{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b01e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4014, 14)\n",
      "4014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.read_csv(\"corpus_JHE.csv\")\n",
    "#print(corpus['text'][0])\n",
    "print(corpus.shape)\n",
    "n = corpus.shape[0]\n",
    "#prints the size of the csv file, the first number is the number of the documents\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a733021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print top words of LDA ()\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for index, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #{}:\".format(index)\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 :-1]])\n",
    "        print(message)\n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2091f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform texts into word-vectors \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Storing the entire training text in a list\n",
    "text = list(corpus.text.values)\n",
    "#these are imposrant parameter settings, \n",
    "#max_df=0.5 means that each token (word) can appear in at most 50% of the documents, \n",
    "#min_df=0.05 means that each token needs to appear in minimum 5% of the documents)\n",
    "tf_vectorizer = CountVectorizer(max_df=0.5, min_df=0.05)\n",
    "# tokenize and build vocab\n",
    "tf_vectorizer.fit(text)\n",
    "#print(tf_vectorizer.vocabulary_)\n",
    "# encode document\n",
    "tf = tf_vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "#print(tf.shape)\n",
    "#print(tf.toarray())\n",
    "#tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a histogram of the most frequent words\n",
    "import numpy as np\n",
    "# Plotly imports\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "feature_names = tf_vectorizer.get_feature_names()\n",
    "count_vec = np.asarray(tf.sum(axis=0)).ravel()\n",
    "zipped = list(zip(feature_names, count_vec))\n",
    "x, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n",
    "# Now I want to extract out on the top 15 and bottom 15 words\n",
    "Y = np.concatenate([y[0:15], y[-16:-1]])\n",
    "X = np.concatenate([x[0:15], x[-16:-1]])\n",
    "\n",
    "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
    "data = [go.Bar(\n",
    "            x = x[0:50],\n",
    "            y = y[0:50],\n",
    "            marker= dict(colorscale='Jet',\n",
    "                         color = y[0:50]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Top 50 Word frequencies after Preprocessing'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')\n",
    "\n",
    "# Plotting the Plot.ly plot for the Top 50 word frequencies\n",
    "data = [go.Bar(\n",
    "            x = x[-100:],\n",
    "            y = y[-100:],\n",
    "            marker= dict(colorscale='Portland',\n",
    "                         color = y[-100:]\n",
    "                        ),\n",
    "            text='Word counts'\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Bottom 100 Word frequencies after Preprocessing'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd74cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual topic modeling is here\n",
    "#from collections import Counter\n",
    "#from scipy.misc import imread\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# n_components=10 -- the number of topics (clusters), change this to get another number of topcis\n",
    "# random_state = 1981 -- this is a random seed in order to have the results reproducible, otherwise, \n",
    "# since the algorithm has random components, the topics may vary slightly from iteration to iteration, \n",
    "# and particularly the order in which topics are displayed may vary)\n",
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter=5,\n",
    "                                learning_method = 'online',\n",
    "                                learning_offset = 50.,\n",
    "                                random_state = 1981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the topic model\n",
    "lda_model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(tf))\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(tf))\n",
    "# See model parameters\n",
    "print(lda_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 30\n",
    "print(\"\\nTopics in LDA model: \")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda_model, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6140f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# Generate a word cloud image for given topic\n",
    "\n",
    "# most important words for each topic\n",
    "vocab = tf_feature_names\n",
    "\n",
    "def draw_word_cloud(index):\n",
    "    imp_words_topic=\"\"\n",
    "    comp=lda_model.components_[index]\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:50]\n",
    "    for word in sorted_words:\n",
    "        imp_words_topic=imp_words_topic+\" \"+word[0]\n",
    "\n",
    "    wordcloud = WordCloud(width=600, height=400).generate(imp_words_topic)\n",
    "    plt.figure( figsize=(5,5))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#change the numbers from 0 to 9 to see all word clouds one-by-one\n",
    "draw_word_cloud(9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print maximum topic of a document\n",
    "doc_topic = lda_model.transform(tf)\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    topic_most_pr = doc_topic[n].argmax()\n",
    "    #print(\"doc: {} topic: {}\\n\".format(n,topic_most_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86701c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tf.shape[0]\n",
    "print(n)\n",
    "# Create Document â€” Topic Matrix\n",
    "lda_output = lda_model.transform(tf)\n",
    "print(lda_output.shape)\n",
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(n)]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic[\"dominant_topic\"] = dominant_topic\n",
    "df_document_topic[\"eid\"] = corpus.eid.values\n",
    "df_document_topic[\"pii\"] = corpus.pii.values\n",
    "#saving document-topic matrix for further analysis\n",
    "df_document_topic.to_csv('dominant_topics.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a topic-keyword matrix as a csv for further analysis\n",
    "df_topic_keywords = pd.DataFrame(lda_model.components_)\n",
    "# Assign Column and Index\n",
    "df_topic_keywords.columns = tf_vectorizer.get_feature_names()\n",
    "df_topic_keywords.index = topicnames\n",
    "df_topic_keywords.to_csv('topics_keywords.csv',index=True)\n",
    "print(df_topic_keywords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment classification, write sentiment scores to csv\n",
    "#(columns in output csv: neg -- fraction of negative sentiment per document, \n",
    "#neu -- fraction of neutral sentiment per document, \n",
    "#pos - fraction of positive sentiment per document,\n",
    "#compound -- overall polarity score which I am not using in the analysis,\n",
    "#eid and pii -- unique identifiers of the articles)\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for x in range(n):\n",
    "    sen = sia.polarity_scores(corpus.text.values[x])\n",
    "    df = pd.DataFrame([sen])\n",
    "    df['eid'] = corpus.eid.values[x]\n",
    "    df['pii'] = corpus.pii.values[x]\n",
    "\n",
    "    if x==0:\n",
    "        sen_all = df\n",
    "    else:\n",
    "        frames = [sen_all, df]\n",
    "        sen_all = pd.concat(frames)\n",
    "\n",
    "sen_all.to_csv('sentiments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de8af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
